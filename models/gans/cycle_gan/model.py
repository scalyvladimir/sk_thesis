from .generators import ResnetGenerator
from .discriminators import NLayerDiscriminator

import pytorch_lightning as pl

import torch.nn.functional as F
import torch.nn as nn
import torchvision
import torch

import functools 

from .utils import init_func

from .loss import GANLoss

import itertools

import random

class GAN(pl.LightningModule):
    def __init__(
        self,
        buffer_size=10,
        is_train=False,
        use_idt=False
    ):
        super().__init__()
        self.save_hyperparameters()
        self.automatic_optimization = False

        # networks

        self.gen_A = ResnetGenerator(
            input_nc=1,
            output_nc=1,
            ngf=64,
            norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False),
            use_dropout=False,
            n_blocks=9
        )
        self.gen_B = ResnetGenerator(
            input_nc=1,
            output_nc=1,
            ngf=64,
            norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False),
            use_dropout=False,
            n_blocks=9
        )

        self.gen_A.apply(init_func)
        self.gen_B.apply(init_func)

        if is_train:
            self.disc_A = NLayerDiscriminator(
                input_nc=1,
                ndf=64,
                n_layers=3,
                norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False),
            )

            self.disc_B = NLayerDiscriminator(
                input_nc=1,
                ndf=64,
                n_layers=3,
                norm_layer=functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False),
            )

            self.disc_A.apply(init_func)
            self.disc_B.apply(init_func)
        
        self.criterion_adv = GANLoss(gan_mode='lsgan')
        self.criterion_cycle = nn.L1Loss()
        self.criterion_idt = nn.L1Loss() if use_idt else None

        self.buffer_A = None
        self.buffer_B = None

        self.buffer_size = buffer_size

    def forward(self, batch):
        batch_from, batch_to = batch

        self.fake_B = self.gen_A(batch_from['img'])  # G_A(A)
        self.rec_A = self.gen_B(self.fake_B)   # G_B(G_A(A))
        self.fake_A = self.gen_B(batch_to['img'])  # G_B(B)
        self.rec_B = self.gen_A(self.fake_A)

    def disc_loss(self, disc, real, fake, name):
        """Calculate GAN loss for the discriminator
        Parameters:
            disc (network)      -- the discriminator D
            real (tensor array) -- real images
            fake (tensor array) -- images generated by a generator
        Return the discriminator loss.
        """

        # Real
        pred_real = disc(real)
        loss_D_real, valid = self.criterion_adv(pred_real, True)
        # Fake
        pred_fake = disc(fake.detach())
        loss_D_fake, fake = self.criterion_adv(pred_fake, False)
        # Combined loss and calculate gradients
        loss_D = (loss_D_real + loss_D_fake) * 0.5

        self.log(f'real_acc_{name}', ((pred_real > 0.5) == valid).cpu().numpy().mean(), on_step=False, on_epoch=True)
        self.log(f'fake_acc_{name}', ((pred_fake > 0.5) == fake).cpu().numpy().mean(), on_step=False, on_epoch=True)
        self.log(f'real_loss_{name}', loss_D_real, on_step=False, on_epoch=True)
        self.log(f'fake_loss_{name}', loss_D_fake, on_step=False, on_epoch=True)
        self.log(f'd_loss_{name}', loss_D, prog_bar=True, on_step=False, on_epoch=True)

        return loss_D
    
    def gen_loss(self, real_A, real_B):
        """Calculate the loss for generators G_A and G_B"""

        # GAN loss D_A(G_A(A))
        loss_gen_A, _ = self.criterion_adv(self.disc_A(self.fake_B), True)
        # GAN loss D_B(G_B(B))
        loss_gen_B, _ = self.criterion_adv(self.disc_B(self.fake_A), True)
        # Forward cycle loss || G_B(G_A(A)) - A||
        loss_cycle_A = self.criterion_cycle(self.rec_A, real_A)
        # Backward cycle loss || G_A(G_B(B)) - B||
        loss_cycle_B = self.criterion_cycle(self.rec_B, real_B)
        # combined loss and calculate gradients
        loss_G = loss_gen_A + loss_gen_B + loss_cycle_A * 10 + loss_cycle_B * 10

        if self.criterion_idt is not None:
            loss_idt_A = self.criterion_idt(self.gen_A(real_B), real_B)
            loss_idt_B = self.criterion_idt(self.gen_B(real_A), real_A)

            loss_G += loss_idt_A * 10 * 0.5 + loss_idt_B * 10 * 0.5 

            self.log('loss_idt_A', loss_idt_A, on_step=False, on_epoch=True)
            self.log('loss_idt_B', loss_idt_B, on_step=False, on_epoch=True)

        self.log('loss_gen_A', loss_gen_A, on_step=False, on_epoch=True)
        self.log('loss_gen_B', loss_gen_B, on_step=False, on_epoch=True)
        self.log('loss_cyc_A', loss_cycle_A, on_step=False, on_epoch=True)
        self.log('loss_cyc_B', loss_cycle_B, on_step=False, on_epoch=True)

        self.log(f'g_loss', loss_G, prog_bar=True, on_step=False, on_epoch=True)

        return loss_G

    def training_step(self, batch):
        """Calculate losses, gradients, and update network weights; called in every training iteration"""
        # forward
        batch_from, batch_to = batch

        self.forward(batch)      # compute fake images and reconstruction images.
        
        optimizer_g, optimizer_d = self.optimizers()

        # G_A and G_B
        self.set_requires_grad([self.disc_A, self.disc_B], False)  # Ds require no gradients when optimizing Gs
        self.toggle_optimizer(optimizer_g, 0)
        optimizer_g.zero_grad()  # set G_A and G_B's gradients to zero
        g_loss = self.gen_loss(batch_from['img'], batch_to['img']) # compute generator loss
        self.manual_backward(g_loss)             # calculate gradients for G_A and G_B
        optimizer_g.step()       # update G_A and G_B's weights
        self.untoggle_optimizer(optimizer_g)
        
        # D_A and D_B
        self.set_requires_grad([self.disc_A, self.disc_B], True)
        self.toggle_optimizer(optimizer_d, 1)
        optimizer_d.zero_grad()   # set D_A and D_B's gradients to zero

        ret_fake_B = []

        if self.buffer_B is None:
            ret_fake_B = self.rec_B
            self.buffer_B = ret_fake_B
        else:
            for rec_sample in self.rec_B:
                if self.buffer_B.shape[0] < self.buffer_size:
                    ret_fake_B.append(rec_sample.unsqueeze(0))
                    self.buffer_B = torch.cat([self.buffer_B, rec_sample.unsqueeze(0)], dim=0)
                else:
                    if random.uniform(0, 1) > 0.5:
                        rand_id = random.randint(0, self.buffer_B.shape[0] - 1)
                        ret_fake_B.append(self.buffer_B[rand_id].unsqueeze(0))
                        self.buffer_B[rand_id] = rec_sample
                    else:
                        ret_fake_B.append(rec_sample.unsqueeze(0))
        
        ret_fake_B = torch.cat(ret_fake_B) if isinstance(ret_fake_B, list) else ret_fake_B

        d_loss_A = self.disc_loss(self.disc_A, real=batch_to['img'], fake=ret_fake_B, name='A')
        self.manual_backward(d_loss_A) # calculate gradients for D_A

        ret_fake_A = []

        if self.buffer_A is None:
            ret_fake_A = self.rec_A
            self.buffer_A = ret_fake_A
        else:
            for rec_sample in self.rec_A:
                if self.buffer_A.shape[0] < self.buffer_size:
                    ret_fake_A.append(rec_sample.unsqueeze(0))
                    self.buffer_A = torch.cat([self.buffer_A, rec_sample.unsqueeze(0)], dim=0)
                else:
                    if random.uniform(0, 1) > 0.5:
                        rand_id = random.randint(0, self.buffer_A.shape[0] - 1)
                        ret_fake_A.append(self.buffer_A[rand_id].unsqueeze(0))
                        self.buffer_A[rand_id] = rec_sample
                    else:
                        ret_fake_A.append(rec_sample.unsqueeze(0))

        ret_fake_A = torch.cat(ret_fake_A) if isinstance(ret_fake_A, list) else ret_fake_A

        d_loss_B = self.disc_loss(self.disc_B, real=batch_from['img'], fake=ret_fake_A, name='B')
        self.manual_backward(d_loss_B) # calculate graidents for D_B
              
        optimizer_d.step()  # update D_A and D_B's weights
        self.untoggle_optimizer(optimizer_d)

    def configure_optimizers(self):

        opt_g = torch.optim.Adam(itertools.chain(self.gen_A.parameters(), self.gen_B.parameters()), betas=(0.5, 0.999), lr=0.0002)
        opt_d = torch.optim.Adam(itertools.chain(self.disc_A.parameters(), self.disc_B.parameters()), betas=(0.5, 0.999), lr=0.0002)
        return [opt_g, opt_d], []

    def validation_step(self, batch, batch_idx):
        batch_from, batch_to = batch

        if batch_idx == 0:
            with torch.no_grad():
                self.forward(batch)

                grid = torchvision.utils.make_grid(torch.cat([batch_from['img'], self.fake_B], dim=0))
                self.logger.log_image('AtoB', [grid], self.current_epoch)

                grid = torchvision.utils.make_grid(torch.cat([batch_to['img'], self.fake_A], dim=0))
                self.logger.log_image('BtoA', [grid], self.current_epoch)

    def set_requires_grad(self, nets, requires_grad=False):
        """Set requies_grad=Fasle for all the networks to avoid unnecessary computations
        Parameters:
            nets (network list)   -- a list of networks
            requires_grad (bool)  -- whether the networks require gradients or not
        """
        if not isinstance(nets, list):
            nets = [nets]
        for net in nets:
            if net is not None:
                for param in net.parameters():
                    param.requires_grad = requires_grad